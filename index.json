[{"body":"This document describes the network topology of Porter in BGP mode and how Porter functions in BGP mode.\nNetwork Topology The following figure shows the topology of the network between a Kubernetes cluster where Porter is installed and a peer BGP router.\nIP addresses and Autonomous System Numbers (ASNs) in the preceding figure are examples only. The topology is described as follows:\n A service backed by two pods is deployed in the Kubernetes cluster, and is assigned an IP address 172.22.0.2 for external access. Porter installed in the Kubernetes cluster establishes a BGP connection with the BGP router, and publishes routes destined for the service to the BGP router. When an external client machine attempts to access the service, the BGP router load balances the traffic among the master, worker 1, and worker 2 nodes based on the routes obtained from Porter. After the service traffic reaches a node, kube-proxy can further forward the traffic to other nodes for load balancing (both pod 1 and pod 2 can be reached over kube-proxy).  Porter uses GoBGP (integrated in Porter) to establish a BGP connection for route publishing. Two CustomResourceDefinitions (CRDs), BgpConf and BgpPeer, are provided for users to configure the local and peer BGP properties on Porter. BgpConf and BgpPeer are designed according to the GoBGP API. For details about how to use BgpConf and BgpPeer to configure Porter in BGP mode, see Configure Porter in BGP Mode.\n","excerpt":"This document describes the network topology of Porter in BGP mode and how Porter functions in BGP …","ref":"/docs/concepts/bgp-mode/","title":"BGP Mode"},{"body":"This document describes how to configure an Eip object, which functions as an IP address pool for Porter both in BGP mode and in Layer 2 mode.\nPorter assigns IP addresses in Eip objects to LoadBalancer services in the Kubernetes cluster. After that, Porter publishes routes destined for the service IP addresses over BGP (in BGP mode), ARP (in Layer 2 mode for IPv4), or NDP (in Layer 2 mode for IPv6).\nNOTE\nCurrently, Porter supports only IPv4 and will soon support IPv6.  Configure an Eip Object for Porter You can create an Eip object to provide an IP address pool for Porter. The following is an example of the Eip YAML configuration:\napiVersion:network.kubesphere.io/v1alpha2kind:Eipmetadata:name:eip-sample-poolspec:address:192.168.0.91-192.168.0.100protocol:layer2interface:eth0disable:falsestatus:occupied:falseusage:1poolSize:10used:\u0026#34;192.168.0.91\u0026#34;: \u0026#34;default/test-svc\u0026#34;firstIP:192.168.0.91lastIP:192.168.0.100ready:truev4:trueThe fields are described as follows:\nmetadata:\n name: Name of the Eip object.  spec:\n  address: One or more IP addresses, which will be used by Porter. The value format can be:\n IP address, for example, 192.168.0.100. IP address/Subnet mask, for example, 192.168.0.0/24. IP address 1-IP address 2, for example, 192.168.0.91-192.168.0.100.  NOTE\nIP segments in different Eip objects cannot overlap. Otherwise, a resource creation error will occur.    protocol: Specifies which mode of Porter the Eip object is used for. The value can be either layer2 or bgp. If this field is not specified, the default value bgp is used.\n  interface: NIC on which Porter listens for ARP or NDP requests. This field is valid only when protocol is set to layer2.\nTIP\nIf the NIC names of the Kubernetes cluster nodes are different, you can set the value to can_reach:IP address (for example, can_reach:192.168.0.5) so that Porter automatically obtains the name of the NIC that can reach the IP address. In this case, you must ensure that the IP address is not used by Kubernetes cluster nodes but can be reached by the cluster nodes.    disable: Specifies whether the Eip object is disabled. The value can be:\n false: Porter can assign IP addresses in the Eip object to new LoadBalancer services. true: Porter stops assigning IP addresses in the Eip object to new LoadBalancer services. Existing services will not be affected.    status: Fields under status specify the status of the Eip object and are automatically configured. When creating an Eip object, you do not need to configure these fields.\n  occupied: Specifies whether IP addresses in the Eip object has been used up.\n  usage: Specifies how many IP addresses in the Eip object have been assigned to services.\n  used: Specifies the used IP addresses and the services that use the IP addresses. The services are displayed in the Namespace/Service name format (for example, default/test-svc).\n  poolSize: Total number of IP addresses in the Eip object.\n  firstIP: First IP address in the Eip object.\n  lastIP: Last IP address in the Eip object.\n  v4: Specifies whether the address family is IPv4. Currently, Porter supports only IPv4 and the value can only be true.\n  ready: Specifies whether the Eip-associated program used for BGP/ARP/NDP routes publishing has been initialized. The program is integrated in Porter.\n  ","excerpt":"This document describes how to configure an Eip object, which functions as an IP address pool for …","ref":"/docs/getting-started/configuration/configure-ip-address-pools-using-eip/","title":"Configure IP Address Pools Using Eip"},{"body":"This document describes how to use kubectl and Helm to install and delete Porter in a Kubernetes cluster. For details about how to install and delete Porter on the KubeSphere web console, see Install Porter on KubeSphere (Web Console).\nPrerequisites   You need to prepare a Kubernetes cluster, and ensure that the Kubernetes version is 1.15 or later. Porter requires CustomResourceDefinition (CRD) v1, which is only supported by Kubernetes 1.15 or later. You can use the following methods to deploy a Kubernetes cluster:\n Use KubeKey (recommended). You can use KubeKey to deploy a Kubernetes cluster with or without KubeSphere. Follow official Kubernetes guides.  Porter is designed to be used in bare-metal Kubernetes environments. However, you can also use a cloud-based Kubernetes cluster for learning and testing.\n  If you use Helm to install porter, ensure that the Helm version is Helm 3.\n  Install Porter Using kubectl   Log in to the Kubernetes cluster over SSH and run the following command:\nkubectl apply -f https://raw.githubusercontent.com/kubesphere/porter/master/deploy/porter.yaml   Run the following command to check whether the status of porter-manager is READY: 1/1 and STATUS: Running. If yes, Porter has been installed successfully.\nkubectl get po -n porter-system   Delete Porter Using kubectl   To delete Porter, log in to the Kubernetes cluster and run the following command:\nkubectl delete -f https://raw.githubusercontent.com/kubesphere/porter/master/deploy/porter.yaml NOTE\nBefore deleting Porter, you must first delete all services that use Porter.    Run the following command to check the result. If the porter-system name space does not exist, Porter has been deleted successfully.\nkubectl get ns   Install Porter Using Helm   Log in to the Kubernetes cluster over SSH and run the following commands:\nhelm repo add test https://charts.kubesphere.io/test helm repo update helm install porter test/porter   Run the following command to check whether the status of porter-manager is READY: 1/1 and STATUS: Running. If yes, Porter has been installed successfully.\nkubectl get po -A   Delete Porter Using Helm   To delete Porter, run the following command:\nhelm delete porter NOTE\nBefore deleting Porter, you must first delete all services that use Porter.    Run the following command to check the result. If the Porter application does not exist, Porter has been deleted successfully.\nhelm ls   ","excerpt":"This document describes how to use kubectl and Helm to install and delete Porter in a Kubernetes …","ref":"/docs/getting-started/installation/install-porter-on-kubernetes/","title":"Install Porter on Kubernetes (kubectl and Helm)"},{"body":"This section describes how to install and delete Porter on Kubernetes and KubeSphere.\n Install Porter on Kubernetes (kubectl and Helm) Describes how to install and delete Porter on Kubernetes using kubectl and Helm.\nInstall Porter on KubeSphere (Web Console) Describes how to install and delete Porter on the KubeSphere web console.\n","excerpt":"This section describes how to install and delete Porter on Kubernetes and KubeSphere.\n Install …","ref":"/docs/getting-started/installation/","title":"Installation"},{"body":"This document describes the network topology of Porter in Layer 2 mode and how Porter functions in Layer 2 mode.\nGenerally, you are advised to use the BGP mode because it allows you to create a high availability system free of failover interruptions and bandwidth bottlenecks. However, BGP may be unavailable in certain systems because of security requirements or because the router does not support BGP. In this case, you can use Porter in Layer 2 mode to achieve similar functionality.\nNetwork Topology The following figure shows the topology of the network between a Kubernetes cluster with Porter and a router.\nIP addresses and MAC addresses in the preceding figure are examples only. The topology is described as follows:\n A service backed by two pods is deployed in the Kubernetes cluster, and is assigned an IP address 192.168.0.91 for external access. The service IP address is on the same network segment as the cluster node IP addresses. Porter installed in the Kubernetes cluster randomly selects a node (worker 1 in this example) to handle service requests. After that, Porter sends an ARP/NDP packet to the router, which maps the service IP address to the MAC address of worker 1. If multiple porter-manager replicas have been deployed in the cluster, Porter uses the the leader election feature of Kubernetes to ensure that only one replica responds to ARP/NDP requests. When an external client machine attempts to access the service, the router forwards the service traffic to worker 1 based on the mapping between the service IP address and the MAC address of worker 1. After the service traffic reaches worker 1, kube-proxy can further forward the traffic to other nodes for load balancing (both pod 1 and pod 2 can be reached over kube-proxy). If worker 1 fails, Porter re-sends an APR/NDP packet to the router to map the service IP address to the MAC address of worker 2, and the service traffic switches to worker 2.  NOTE\nThe Layer 2 mode has two limitations:\n Worker 1 and worker 2 work in active-standby mode in terms of traffic forwarding. When a failover occurs, services in the Kubernetes cluster will be interrupted for a short while. All service traffic is always sent to one node first and then forwarded to other nodes over kube-proxy in a second hop. Therefore, the service bandwidth is limited to the bandwidth of a single node, which causes a bandwidth bottleneck.   ","excerpt":"This document describes the network topology of Porter in Layer 2 mode and how Porter functions in …","ref":"/docs/concepts/layer-2-mode/","title":"Layer 2 Mode"},{"body":"Porter is an open-source load balancer implementation designed for bare-metal Kubernetes clusters.\nWhy Porter In cloud-based Kubernetes clusters, services are usually exposed by using load balancers provided by cloud vendors. However, cloud-based load balancers are unavailable in bare-metal environments. Porter allows users to create LoadBalancer services in bare-metal environments for external access, and provides the same user experience as cloud-based load balancers.\nCore Features  BGP mode and Layer 2 mode ECMP routing and load balancing IP address pool management BGP configuration using CRDs Installation using Helm and KubeSphere  Support, Discussion and Contributing Porter is a sub-project of KubeSphere.\n Join us at the KubeSphere Slack Channel to get support or simply tell us that you are using Porter. You have code or documents for Porter? We ❤️ all sorts of contributions! You can build the Porter project and send pull requests to the GitHub Porter Repository.  Landscapes  \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;  Porter is a promising newcomer in service proxy, which enriches the CNCF CLOUD NATIVE Landscape.  License Porter is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.\n","excerpt":"Porter is an open-source load balancer implementation designed for bare-metal Kubernetes clusters. …","ref":"/docs/overview/","title":"Overview"},{"body":"This document demonstrates how to use Porter in BGP mode to expose a service backed by two pods. The BgpConf, BgpPeer, Eip, deployment and service described in this document are examples only and you need to customize the commands and YAML configurations based on your requirements.\nInstead of using a real router, this document uses a Linux server with BIRD to simulate a router so that users without a real router can also use Porter in BGP mode for tests.\nPrerequisites   You need to prepare a Kubernetes cluster where Porter has been installed. For details, see Install Porter on Kubernetes (kubectl and Helm) and Install Porter on KubeSphere (Web Console).\n  You need to prepare a Linux server that communicates with the Kubernetes cluster properly. BIRD will be installed on the server to simulate a BGP router.\n  If you use a real router instead of BIRD, the router must support BGP and Equal-Cost Multi-Path (ECMP) routing. In addition, the router must also support receiving multiple equivalent routes from the same neighbor.\n  This document uses the following devices as an example:\n   Device Name IP Address Description     master1 192.168.0.2 Kubernetes cluster master, where Porter is installed.   worker-p001 192.168.0.3 Kubernetes cluster worker 1   worker-p002 192.168.0.4 Kubernetes cluster worker 2   i-f3fozos0 192.168.0.5 BIRD machine, where BIRD will be installed to simulate a BGP router.    Step 1: Install and Configure BIRD If you use a real router, you can skip this step and perform configuration on the router instead.\n  Log in to the BIRD machine and run the following commands to install BIRD:\nsudo add-apt-repository ppa:cz.nic-labs/bird sudo apt-get update sudo apt-get install bird sudo systemctl enable bird NOTE\n BIRD 1.5 does not support ECMP. To use all features of Porter, you are advised to install BIRD 1.6 or later. The preceding commands apply only to Debian-based OSs such as Debian and Ubuntu. On Red Hat-based OSs such as RHEL and CentOS, use yum instead. You can also install BIRD according to the official BIRD documentation.     Run the following command to edit the BIRD configuration file:\nvi /etc/bird/bird.conf   Configure the BIRD configuration file as follows:\nrouter id 192.168.0.5; protocol kernel { scan time 60; import none; export all; merge paths on; } protocol bgp neighbor1 { local as 50001; neighbor 192.168.0.2 port 17900 as 50000; source address 192.168.0.5; import all; export all; enable route refresh off; add paths on; } NOTE\n  For test usage, you only need to customize the following fields in the preceding configuration:\nrouter id: Router ID of the BIRD machine, which is usually set to the IP address of the BIRD machine.\nprotocol bgp neighbor1:\n local as: ASN of the BIRD machine, which must be different from the ASN of the Kubernetes cluster. neighbor: Master node IP address, BGP port number, and ASN of the Kubernetes cluster. Use port 17900 instead of the default BGP port 179 to avoid conflicts with other BGP components in the system. source address: IP address of the BIRD machine.    If multiple nodes in the Kubernetes are used as BGP neighbors, you need to configure multiple BGP neighbors in the BIRD configuration file.\n  For details about the BIRD configuration file, see the official BIRD documentation.\n      Run the following command to restart BIRD:\nsudo systemctl restart bird   Run the following command to check whether the status of BIRD is active:\nsudo systemctl status bird NOTE\nIf the status of BIRD is not active, you can run the following command to check the error logs:\njournalctl -f -u bird     Step 2: Create a BgpConf Object The BgpConf object is used to configure the local (Kubernetes cluster) BGP properties on Porter.\n  Run the following command to create a YAML file for the BgpConf object:\nvi porter-bgp-conf.yaml   Add the following information to the YAML file:\napiVersion:network.kubesphere.io/v1alpha2kind:BgpConfmetadata:name:defaultspec:as:50000listenPort:17900routerId:192.168.0.2NOTE\nFor details about the fields in the BgpConf YAML configuration, see Configure Local BGP Properties Using BgpConf.    Run the following command to create the BgpConf object:\nkubectl apply -f porter-bgp-conf.yaml   Step 3: Create a BgpPeer Object The BgpPeer object is used to configure the peer (BIRD machine) BGP properties on Porter.\n  Run the following command to create a YAML file for the BgpPeer object:\nvi porter-bgp-peer.yaml   Add the following information to the YAML file:\napiVersion:network.kubesphere.io/v1alpha2kind:BgpPeermetadata:name:porter-bgp-peerspec:conf:peerAs:50001neighborAddress:192.168.0.5NOTE\nFor details about the fields in the BgpPeer YAML configuration, see Configure Peer BGP Properties Using BgpPeer.    Run the following command to create the BgpPeer object:\nkubectl apply -f porter-bgp-peer.yaml   Step 4: Create an Eip Object The Eip object functions as an IP address pool for Porter.\n  Run the following command to create a YAML file for the Eip object:\nvi porter-bgp-eip.yaml   Add the following information to the YAML file:\napiVersion:network.kubesphere.io/v1alpha2kind:Eipmetadata:name:porter-bgp-eipspec:address:172.22.0.2-172.22.0.10NOTE\nFor details about the fields in the Eip YAML configuration, see Configure IP Address Pools Using Eip.    Run the following command to create the Eip object:\nkubectl apply -f porter-bgp-eip.yaml   Step 5: Create a Deployment The following creates a deployment of two pods using the luksa/kubia image. Each pod returns its own pod name to external requests.\n  Run the following command to create a YAML file for the deployment:\nvi porter-bgp.yaml   Add the following information to the YAML file:\napiVersion:apps/v1kind:Deploymentmetadata:name:porter-bgpspec:replicas:2selector:matchLabels:app:porter-bgptemplate:metadata:labels:app:porter-bgpspec:containers:- image:luksa/kubianame:kubiaports:- containerPort:8080  Run the following command to create the deployment:\nkubectl apply -f porter-bgp.yaml   Step 6: Create a Service   Run the following command to create a YAML file for the service:\nvi porter-bgp-svc.yaml   Add the following information to the YAML file:\nkind:ServiceapiVersion:v1metadata:name:porter-bgp-svcannotations:lb.kubesphere.io/v1alpha1:porterprotocol.porter.kubesphere.io/v1alpha1:bgpeip.porter.kubesphere.io/v1alpha2:porter-bgp-eipspec:selector:app:porter-bgptype:LoadBalancerports:- name:httpport:80targetPort:8080externalTrafficPolicy:ClusterNOTE\n You must set spec.type to LoadBalancer. The lb.kubesphere.io/v1alpha1: porter annotation specifies that the service uses Porter. The protocol.porter.kubesphere.io/v1alpha1: bgp annotation specifies that Porter is used in BGP mode. The eip.porter.kubesphere.io/v1alpha2: porter-bgp-eip annotation specifies the Eip object used by Porter. If this annotation is not configured, Porter automatically uses the first available Eip object that matches the protocol. You can also delete this annotation and add the spec.loadBalancerIP field (for example, spec.loadBalancerIP: 172.22.0.2) to assign a specific IP address to the service. In the BGP mode, you can set spec.loadBalancerIP of multiple services to the same value for IP address sharing (the services are distinguished by different service ports). In this case, you must set spec.ports.port to different values and spec.externalTrafficPolicy to Cluster for the services. If spec.externalTrafficPolicy is set to Cluster (default value), Porter uses all Kubernetes cluster nodes as the next hops destined for the service. If spec.externalTrafficPolicy is set to Local, Porter uses only Kubernetes cluster nodes that contain pods as the next hops destined for the service.     Run the following command to create the service:\nkubectl apply -f porter-bgp-svc.yaml   Step 7: Verify Porter in BGP Mode The following verifies whether Porter functions properly.\n  In the Kubernetes cluster, run the following command to obtain the external IP address of the service:\nkubectl get svc   In the Kubernetes cluster, run the following command to obtain the IP addresses of the cluster nodes:\n  On the BIRD machine, run the following command to check the routing table. If equivalent routes using the Kubernetes cluster nodes as next hops destined for the service are displayed, Porter functions properly.\nip route If spec.externalTrafficPolicy in the service YAML configuration is set to Cluster, all Kubernetes cluster nodes are used as the next hops.\nIf spec.externalTrafficPolicy in the service YAML configuration is set to Local, only Kubernetes cluster nodes that contain pods are used as the next hops.\n  On the BIRD machine, run the following command to access the service:\ncurl 172.22.0.2   ","excerpt":"This document demonstrates how to use Porter in BGP mode to expose a service backed by two pods. The …","ref":"/docs/getting-started/usage/use-porter-in-bgp-mode/","title":"Use Porter in BGP Mode"},{"body":"This section describes the basic concepts of Porter.\n BGP Mode Describes how Porter functions in BGP mode.\nLayer 2 Mode Describes how Porter functions in Layer 2 mode.\n","excerpt":"This section describes the basic concepts of Porter.\n BGP Mode Describes how Porter functions in BGP …","ref":"/docs/concepts/","title":"Concepts"},{"body":"This section describes how to perform configuration before using Porter.\n Configure IP Address Pools Using Eip Describes how to configure IP address pools for Porter using Eip.\nConfigure Porter in BGP Mode Describes how to configure local and peer BGP properties on Porter using BgpConf and BgpPeer.\nConfigure Porter for Multi-Router Clusters Describes how to configure Porter for multi-router Kubernetes clusters.\nConfigure Multiple Porter Replicas Describes how to configure multiple Porter replicas to ensure high availability.\n","excerpt":"This section describes how to perform configuration before using Porter.\n Configure IP Address Pools …","ref":"/docs/getting-started/configuration/","title":"Configuration"},{"body":"This document describes how to configure Porter in BGP mode. If Porter is used in Layer 2 mode, you do not need to configure Porter.\nConfigure Local BGP Properties Using BgpConf You can create a BgpConf object in the Kubernetes cluster to configure the local BGP properties on Porter. The following is an example of the BgpConf YAML configuration:\napiVersion:network.kubesphere.io/v1alpha2kind:BgpConfmetadata:name:defaultspec:as:50000listenPort:17900routerId:192.168.0.2The fields are described as follows:\nmetadata:\n name: BgpConf object name. Porter recognizes only the name default. BgpConf objects with other names will be ignored.  spec:\n as: Local ASN, which must be different from the value of spec.conf.peerAS in the BgpPeer configuration. listenPort: Port on which Porter listens. The default value is 179 (default BGP port number). If other components (such as Calico) in the Kubernetes cluster also use BGP and port 179, you must set a different value to avoid the conflict. routerID: Local router ID, which is usually set to the IP address of the master NIC of the Kubernetes master node. If this field is not specified, the first IP address of the node where porter-manager is located will be used.  Configure Peer BGP Properties Using BgpPeer You can create a BgpPeer object in the Kubernetes cluster to configure the peer BGP properties on Porter. The following is an example of the BgpPeer YAML configuration:\napiVersion:network.kubesphere.io/v1alpha2kind:BgpPeermetadata:name:bgppeer-samplespec:conf:peerAs:50001neighborAddress:192.168.0.5afiSafis:- config:family:afi:AFI_IPsafi:SAFI_UNICASTenabled:trueaddPaths:config:sendMax:10nodeSelector:matchLabels:porter.kubesphere.io/rack:leaf1The fields are described as follows:\nmetadata:\n name: Name of the BgpPeer object. If there are multiple peer BGP routers, you can create multiple BgpPeer objects with different names.  spec.conf:\n peerAS: ASN of the peer BGP router, which must be different from the value of spec.as in the BgpConf configuration. neighborAddress: IP address of the peer BGP router.  spec.afiSafis.addPaths.config:\n sendMax: Maximum number of equivalent routes that Porter can send to the peer BGP router for Equal-Cost Multi-Path (ECMP) routing. The default value is 10.  spec.nodeSelector.matchLabels:\n porter.kubesphere.io/rack: If the Kubernetes cluster nodes are deployed under different routers and each node has one Porter replica, you need to configure this field so that the Porter replica on the correct node establishes a BGP connection with the peer BGP router. By default, all porter-manager replicas will respond to the BgpPeer configuration and attempt to establish a BGP connection with the peer BGP router.  Other fields under spec.afiSafis specify the address family. Currently, Porter supports only IPv4 and you can directly use the values in the example configuration.\n","excerpt":"This document describes how to configure Porter in BGP mode. If Porter is used in Layer 2 mode, you …","ref":"/docs/getting-started/configuration/configure-porter-in-bgp-mode/","title":"Configure Porter in BGP Mode"},{"body":"This document describes how to install and delete Porter on the KubeSphere web console. For details about how to install and delete Porter in a Kubernetes cluster without KubeSphere, see Install Porter on Kubernetes (kubectl and Helm).\nPrerequisites You need to prepare a Kubernetes cluster with KubeSphere, and ensure that the Kubernetes version is 1.15 or later. Porter requires CustomResourceDefinition (CRD) v1, which is only supported by Kubernetes 1.15 or later. You can use the following methods to install KubeSphere:\n Deploy a new Kubernetes cluster with KubeSphere. Install KubeSphere in an existing Kubernetes cluster.  Porter is designed to be used in bare-metal Kubernetes environments. However, you can also use a cloud-based Kubernetes cluster for learning and testing.\nInstall Porter on the KubeSphere Web Console   Log in to the KubeSphere console and go to your workspace.\n  On the left navigation bar, choose Apps Management \u0026gt; App Repos, and click Add Repo on the right.\n  In the displayed dialog box, set App Repository Name (for example, KubeSphere-test), set URL to https://charts.kubesphere.io/test, click Validate to check the URL, and click OK.\n  Go to your project, choose Application Workloads \u0026gt; Applications on the left navigation bar, and click Deploy New Application on the right.\n  In the displayed dialog box, click From App Templates.\n  Select KubeSphere-test from the drop-down list and click porter.\n  Click Deploy and follow the wizard instructions to complete the installation. You can customize the chart configuration in the YAML file based on your requirements.\n  Choose Application Workloads \u0026gt; Pods on the left navigation bar to check whether the status of porter-manager is running. If yes, porter has been installed successfully.\n  Delete Porter on the KubeSphere Web Console To delete Porter on the KubeSphere web console, go to your project, choose Application Workloads \u0026gt; Applications on the left navigation bar, click on the right of the Porter application, and choose Delete from the drop-down list.\nNOTE\nBefore deleting Porter, you must first delete all services that use Porter.  ","excerpt":"This document describes how to install and delete Porter on the KubeSphere web console. For details …","ref":"/docs/getting-started/installation/install-porter-on-kubesphere/","title":"Install Porter on KubeSphere (Web Console)"},{"body":"This document demonstrates how to use Porter in Layer 2 mode to expose a service backed by two pods. The Eip, deployment and service described in this document are examples only and you need to customize the commands and YAML configurations based on your requirements.\nPrerequisites  You need to prepare a Kubernetes cluster where Porter has been installed. For details, see Install Porter on Kubernetes (kubectl and Helm) and Install Porter on KubeSphere (Web Console). All Kubernetes cluster nodes must be on the same Layer 2 network (under the same router). You need to prepare a client machine, which is used to verify whether Porter functions properly in Layer 2 mode. The client machine needs to be on the same network as the Kubernetes cluster nodes.  This document uses the following devices as an example:\n   Device Name IP Address MAC Address Description     master1 192.168.0.2 52:54:22:a3:9a:d9 Kubernetes cluster master   worker-p001 192.168.0.3 52:54:22:3a:e6:6e Kubernetes cluster worker 1   worker-p002 192.168.0.4 52:54:22:37:6c:7b Kubernetes cluster worker 2   i-f3fozos0 192.168.0.5 52:54:22:fa:b9:3b Client machine    Step 1: Enable strictARP for kube-proxy In Layer 2 mode, you need to enable strictARP for kube-proxy so that all NICs in the Kubernetes cluster stop answering ARP requests from other NICs and Porter handles ARP requests instead.\n  Log in to the Kubernetes cluster and run the following command to edit the kube-proxy ConfigMap:\nkubectl edit configmap kube-proxy -n kube-system   In the kube-proxy ConfigMap YAML configuration, set data.config.conf.ipvs.strictARP to true.\nipvs:strictARP:true  Run the following command to restart kube-proxy:\nkubectl rollout restart daemonset kube-proxy -n kube-system   Step 2: Specify the NIC Used for Porter If the node where Porter is installed has multiple NICs, you need to specify the NIC used for Porter in Layer 2 mode. You can skip this step if the node has only one NIC.\nIn this example, the master1 node where Porter is installed has two NICs (eth0 192.168.0.2 and eth1 192.168.1.2), and eth0 192.168.0.2 will be used for Porter.\nRun the following command to annotate master1 to specify the NIC:\nkubectl annotate nodes master1 layer2.porter.kubesphere.io/v1alpha1=\u0026#34;192.168.0.2\u0026#34; Step 3: Create an Eip Object The Eip object functions as an IP address pool for Porter.\n  Run the following command to create a YAML file for the Eip object:\nvi porter-layer2-eip.yaml   Add the following information to the YAML file:\napiVersion:network.kubesphere.io/v1alpha2kind:Eipmetadata:name:porter-layer2-eipspec:address:192.168.0.91-192.168.0.100interface:eth0protocol:layer2NOTE\n  The IP addresses specified in spec.address must be on the same network segment as the Kubernetes cluster nodes.\n  For details about the fields in the Eip YAML configuration, see Configure IP Address Pools Using Eip.\n      Run the following command to create the Eip object:\nkubectl apply -f porter-layer2-eip.yaml   Step 4: Create a Deployment The following creates a deployment of two pods using the luksa/kubia image. Each pod returns its own pod name to external requests.\n  Run the following command to create a YAML file for the deployment:\nvi porter-layer2.yaml   Add the following information to the YAML file:\napiVersion:apps/v1kind:Deploymentmetadata:name:porter-layer2spec:replicas:2selector:matchLabels:app:porter-layer2template:metadata:labels:app:porter-layer2spec:containers:- image:luksa/kubianame:kubiaports:- containerPort:8080  Run the following command to create the deployment:\nkubectl apply -f porter-layer2.yaml   Step 5: Create a Service   Run the following command to create a YAML file for the service:\nvi porter-layer2-svc.yaml   Add the following information to the YAML file:\nkind:ServiceapiVersion:v1metadata:name:porter-layer2-svcannotations:lb.kubesphere.io/v1alpha1:porterprotocol.porter.kubesphere.io/v1alpha1:layer2eip.porter.kubesphere.io/v1alpha2:porter-layer2-eipspec:selector:app:porter-layer2type:LoadBalancerports:- name:httpport:80targetPort:8080externalTrafficPolicy:ClusterNOTE\n You must set spec.type to LoadBalancer. The lb.kubesphere.io/v1alpha1: porter annotation specifies that the service uses Porter. The protocol.porter.kubesphere.io/v1alpha1: layer2 annotation specifies that Porter is used in Layer 2 mode. The eip.porter.kubesphere.io/v1alpha2: porter-layer2-eip annotation specifies the Eip object used by Porter. If this annotation is not configured, Porter automatically uses the first available Eip object that matches the protocol. You can also delete this annotation and add the spec.loadBalancerIP field (for example, spec.loadBalancerIP: 192.168.0.91) to assign a specific IP address to the service. If spec.externalTrafficPolicy is set to Cluster (default value), Porter randomly selects a node from all Kubernetes cluster nodes to handle service requests. Pods on other nodes can also be reached over kube-proxy. If spec.externalTrafficPolicy is set to Local, Porter randomly selects a node that contains a pod in the Kubernetes cluster to handle service requests. Only pods on the selected node can be reached.     Run the following command to create the service:\nkubectl apply -f porter-layer2-svc.yaml   Step 6: Verify Porter in Layer 2 Mode The following verifies whether Porter functions properly.\n  In the Kubernetes cluster, run the following command to obtain the external IP address of the service:\nkubectl get svc porter-layer2-svc   In the Kubernetes cluster, run the following command to obtain the IP addresses of the cluster nodes:\nkubectl get nodes -o wide   In the Kubernetes cluster, run the following command to check the nodes of the pods:\nkubectl get po NOTE\nIn this example, the pods are automatically assigned to different nodes. You can manually assign pods to different nodes.    On the client machine, run the following commands to ping the service IP address and check the IP neighbors:\nping 192.168.0.91 ip neigh In the output of the ip neigh command, the MAC address of the service IP address 192.168.0.91 is the same as that of worker-p001 192.168.0.3. Therefore, Porter has mapped the service IP address to the MAC address of worker-p001.\n  On the client machine, run the following command to access the service:\ncurl 192.168.0.91 If spec.externalTrafficPolicy in the service YAML configuration is set to Cluster, both pods can be reached.\nIf spec.externalTrafficPolicy in the service YAML configuration is set to Local, only the pod on the node selected by Porter can be reached.\n  ","excerpt":"This document demonstrates how to use Porter in Layer 2 mode to expose a service backed by two pods. …","ref":"/docs/getting-started/usage/use-porter-in-layer-2-mode/","title":"Use Porter in Layer 2 Mode"},{"body":"This document describes how to configure Porter in BGP mode for Kubernetes cluster nodes deployed under multiple routers. You can skip this document if all Kubernetes cluster nodes are deployed under the same router.\nNOTE\nThis document applies only to the BGP mode. The Layer 2 mode requires that all Kubernetes cluster nodes be on the same Layer 2 network (under the same router).  Network Topology Before Configuration This section explains why you need to perform the configuration. The following figure shows the network topology of a Kubernetes cluster before the configuration.\nIP addresses in the preceding figure are examples only. The topology is described as follows:\n In the Kubernetes cluster, the master and worker 1 nodes are deployed under the leaf 1 BGP router, and the worker 2 node is deployed under the leaf 2 BGP router. Porter is only installed under leaf 1 (by default, only one Porter replica is installed). A service backed by two pods is deployed in the Kubernetes cluster, and is assigned an IP address 172.22.0.2 for external access. Pod 1 and pod 2 are deployed on worker 1 and worker 2 respectively. Porter establishes a BGP connection with leaf 1 and publishes the IP addresses of the master node and worker 1 (192.168.0.3 and 192.168.0.4) to leaf 1 as the next hop destined for the service IP address 172.22.0.2. Leaf 1 establishes a BGP connection with the spine BGP router and publishes its own IP address 192.168.0.2 to the spine router as the next hop destined for the service IP address 172.22.0.2. When an external client machine attempts to access the service, the spine router forwards the service traffic to leaf 1, and leaf 1 load balances the traffic among the master node and worker 1. Although pod 2 on worker 2 can also be reached over kube-proxy, router-level load balancing is implemented only among the master node and worker 1 and the service bandwidth is limited to the bandwidth of the master node and worker 1.  To resolve the problem, you need to label the Kubernetes cluster nodes and change the Porter deployment configuration so that Porter is installed on nodes under all leaf routers. In addition, you need to specify the spec.nodeSelector.matchLabels field in the BgpPeer configuration so that the Porter replicas establish BGP connections with the correct BGP routers.\nNetwork Topology After Configuration This section describes the configuration result you need to achieve. The following figure shows the network topology of a Kubernetes cluster after the configuration.\nIP addresses in the preceding figure are examples only. The topology is described as follows:\n After the configuration, Porter is installed on nodes under all leaf routers. In addition to what happens before the configuration, the Porter replica installed under leaf 2 also establishes a BGP connection with leaf 2 and publishes the worker 2 IP address 192.168.1.2 to leaf 2 as the next hop destined for the service IP address 172.22.0.2. Leaf 2 establishes a BGP connection with the spine router and publishes its own IP address 192.168.1.1 to the spine router as the next hop destined for the service IP address 172.22.0.2. When an external client machine attempts to access the service, the spine router load balances the service traffic among leaf 1 and leaf 2. Leaf 1 load balances the traffic among the master node and worker 1. Leaf 2 forwards the traffic to worker 2. Therefore, the service traffic is load balanced among all three Kubernetes cluster nodes, and the service bandwidth of all three nodes can be utilized.  Configuration Procedure Prerequisites You need to prepare a Kubernetes cluster where Porter has been installed. For details, see Install Porter on Kubernetes (kubectl and Helm) and Install Porter on KubeSphere (Web Console).\nProcedure NOTE\nThe node names, leaf router names, and namespace in the following steps are examples only. You need to use the actual values in your environment.    Log in to the Kubernetes cluster and run the following commands to label the Kubernetes cluster nodes where Porter is to be installed:\nkubectl label --overwrite nodes master1 worker-p002 lb.kubesphere.io/v1alpha1=porter NOTE\nPorter works properly if it is installed on only one node under each leaf router. In this example, Porter will be installed on master1 under leaf1 and worker-p002 under leaf2. However, to ensure high availability in a production environment, you are advised to installed Porter on at least two nodes under each leaf router.    Run the following command to scale the number of porter-manager pods to 0:\nkubectl scale deployment porter-manager --replicas=0 -n porter-system   Run the following command to edit the porter-manager deployment:\nkubectl edit deployment porter-manager -n porter-system   In the porter-manager deployment YAML configuration, add the following fields under spec.template.spec:\nnodeSelector:kubernetes.io/os:linuxlb.kubesphere.io/v1alpha1:porter  Run the following command to scale the number of porter-manager pods to the required number (change the number 2 to the actual value):\nkubectl scale deployment porter-manager --replicas=2 -n porter-system   Run the following command to check whether Porter has been installed on the required nodes.\nkubectl get po -n porter-system -o wide   Run the following commands to label the Kubernetes cluster nodes so that the Porter replicas establish BGP connections with the correct BGP routers.\nkubectl label --overwrite nodes master1 porter.kubesphere.io/rack=leaf1 kubectl label --overwrite nodes worker-p002 porter.kubesphere.io/rack=leaf2   When creating BgpPeer objects, configure the spec.nodeSelector.matchLabels field in the BgpPeer YAML configuration for each leaf router. The following YAML configurations specify that the Porter replica on master1 communicates with leaf1, and the Porter replica on worker-p002 communicates with leaf2.\n# BgpPeer YAML for master1 and leaf1nodeSelector:matchLabels:porter.kubesphere.io/rack:leaf1# BgpPeer YAML for worker-p002 and leaf2nodeSelector:matchLabels:porter.kubesphere.io/rack:leaf2  ","excerpt":"This document describes how to configure Porter in BGP mode for Kubernetes cluster nodes deployed …","ref":"/docs/getting-started/configuration/configure-porter-for-multi-router-clusters/","title":"Configure Porter for Multi-router Clusters (BGP Mode)"},{"body":"This section describes how to install, configure, and use Porter.\n Installation Describes how to install Porter on Kubernetes and KubeSphere.\nConfiguration Describes how to perform configuration before using Porter.\nUsage Describes how to use Porter.\n","excerpt":"This section describes how to install, configure, and use Porter.\n Installation Describes how to …","ref":"/docs/getting-started/","title":"Getting Started"},{"body":"This section describes how to use Porter in BGP mode and Layer 2 mode.\n Use Porter in BGP Mode Describes how to use Porter in BGP mode.\nUse Porter in Layer 2 Mode Describes how to use Porter in Layer 2 mode.\n","excerpt":"This section describes how to use Porter in BGP mode and Layer 2 mode.\n Use Porter in BGP Mode …","ref":"/docs/getting-started/usage/","title":"Usage"},{"body":"This document describes how to build the Porter project.\nPrerequisites  You need to prepare a Linux environment. You need to install Go 1.12 or later. You need to install Docker. You need to install Docker Buildx.  Procedure   Log in to your environment, and run the following commands to clone the Porter project and go to the porter directory:\ngit clone https://github.com/kubesphere/porter.git cd porter   Run the following command to install Kustomize and Kubebuilder:\n./hack/install_tools.sh   Run the following command to install controller-gen:\ngo get sigs.k8s.io/controller-tools/cmd/controller-gen@v0.4.0   Run the following command to configure the environment variable for controller-gen:\nexport PATH=/root/go/bin/:$PATH NOTE\nYou need to change /root/go/bin/ to the actual path of controller-gen.    Run the following command to generate CRDs and webhooks:\nmake generate   Customize the values of IMG_MANAGER and IMG_AGENT in Makefile and run the following command to generate a YAML release file in the deploy directory:\nmake release NOTE\n  IMG_MANAGER specifies the repository and tag of the porter-manager image.\n  IMG_AGENT specifies the repository and tag of the porter-agent image.\n  Currently, Porter uses only the porter-manager image. The porter-agent image will be used in future versions.\n      Run the following command to deploy Porter as a plugin:\nkubectl apply -f deploy/release.yaml   ","excerpt":"This document describes how to build the Porter project.\nPrerequisites  You need to prepare a Linux …","ref":"/docs/building/","title":"Building"},{"body":"This document describes how to configure multiple Porter replicas to ensure high availability in a production environment. You can skip this document if Porter is used in a test environment. By default, only one Porter replica is installed in a Kubernetes cluster.\n If all Kubernetes cluster nodes are deployed under the same router (BGP mode or Layer 2 mode), you are advised to configure at least two Porter replicas, which are installed on two Kubernetes cluster nodes respectively. If the Kubernetes cluster nodes are deployed under different leaf routers (BGP mode only), you are advised to configure at least two Porter replicas (one replica for one node) under each leaf router. For details, see Configure Porter for Multi-router Clusters.  Prerequisites You need to prepare a Kubernetes cluster where Porter has been installed. For details, see Install Porter on Kubernetes (kubectl and Helm) and Install Porter on KubeSphere (Web Console).\nProcedure NOTE\nThe node names and namespace in the following steps are examples only. You need to use the actual values in your environment.    Log in to the Kubernetes cluster and run the following command to label the Kubernetes cluster nodes where Porter is to be installed:\nkubectl label --overwrite nodes master1 worker-p002 lb.kubesphere.io/v1alpha1=porter NOTE\nIn this example, Porter will be installed on master1 and worker-p002.    Run the following command to scale the number of porter-manager pods to 0:\nkubectl scale deployment porter-manager --replicas=0 -n porter-system   Run the following command to edit the porter-manager deployment:\nkubectl edit deployment porter-manager -n porter-system   In the porter-manager deployment YAML configuration, add the following fields under spec.template.spec:\nnodeSelector:kubernetes.io/os:linuxlb.kubesphere.io/v1alpha1:porter  Run the following command to scale the number of porter-manager pods to the required number (change the number 2 to the actual value):\nkubectl scale deployment porter-manager --replicas=2 -n porter-system   Run the following command to check whether Porter has been installed on the required nodes.\nkubectl get po -n porter-system -o wide   NOTE\n In Layer 2 mode, Porter uses the leader election feature of Kubernetes to ensure that only one replica responds to ARP/NDP requests. In BGP mode, all Porter replicas will respond to the BgpPeer configuration and attempt to establish a BGP connection with the peer BGP router by default. If the Kubernetes cluster nodes are deployed under different routers, you need to perform further configuration so that the Porter replicas establish BGP connections with the correct BGP routers. For details, see Configure Porter for Multi-router Clusters.   ","excerpt":"This document describes how to configure multiple Porter replicas to ensure high availability in a …","ref":"/docs/getting-started/configuration/configure-multiple-porter-replicas/","title":"Configure Multiple Porter Replicas"},{"body":"Follow the documentation to learn how to use Porter.\n Overview Introduces Porter.\nConcepts Describes the basic concepts of Porter.\nGetting Started Describes how to install, configure, and use Porter.\nBuilding Describes how to build the Porter project.\n","excerpt":"Follow the documentation to learn how to use Porter.\n Overview Introduces Porter.\nConcepts Describes …","ref":"/docs/","title":"Documentation"},{"body":"The News of Porter project.\n","excerpt":"The News of Porter project.","ref":"/blog/news/","title":"News About Docsy"},{"body":"As we know, the backend workload can be exposed externally using service of type \u0026ldquo;LoadBalancer\u0026rdquo; in Kubernetes cluster. Cloud vendors often provide cloud LB plugins for Kubernetes which requires the cluster to be deployed on a specific IaaS platform. However, many enterprise users usually deploy Kubernetes clusters on bare meta especially for production use. For the on-premise bare meta clusters, Kubernetes does not provide Load-Balancer implementation. Porter, an open-source project, is the right solution for such issue.\nThis video will focus on the network technologies to help expose service and EIP management for bare meta Kubernetes cluster.\n ","excerpt":"As we know, the backend workload can be exposed externally using service of type …","ref":"/blog/2019/06/24/an-open-source-load-balancer-for-bare-metal-kubernetes/","title":"An Open Source Load Balancer for Bare Metal Kubernetes"},{"body":"On this livestream from KubeCon + CloudNativeCon China, Alex Williams was sitting down with Xuetao Song, Senior Software Engineer at Beijing Yunify Technology Co., Ltd. and Fang (Flora) Du, QingCloud Solution Architect at Beijing Yunify Technology Co., Ltd. to discuss open source load balancing on bare metal. Porter exists as an OSS solution to the issue of load balancing on bare metal in production on Kubernetes, which Song and Du are giving a presentation on at KCCNC + OSS Summit China 2019.\n Related Post Please reference How a China-Based Bare Metal Service Provider Tackled Kubernetes Load Balancing for details.\n","excerpt":"On this livestream from KubeCon + CloudNativeCon China, Alex Williams was sitting down with Xuetao …","ref":"/blog/2019/06/24/the-interview-to-porter-from-the-new-stack/","title":"The Interview to Porter from The New Stack"},{"body":"","excerpt":"","ref":"/index.json","title":""},{"body":"This is the blog section, including News and Releases.\n","excerpt":"This is the blog section, including News and Releases.","ref":"/blog/","title":"Docsy Blog"},{"body":"","excerpt":"","ref":"/community/","title":"Community"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu394e7663bf611d6303fc6d5151c4565c_47376_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu394e7663bf611d6303fc6d5151c4565c_47376_1920x1080_fill_q75_catmullrom_top.jpg); } }  Porter: Load Balancer Implementation for Bare-Metal Kubernetes Clusters Learn More   Download   Expose your LoadBalancer services in bare-metal Kubernetes clusters\n\n        Why Porter? In cloud-based Kubernetes clusters, services are usually exposed by using load balancers provided by cloud vendors. However, cloud-based load balancers are unavailable in bare-metal environments. Porter allows users to create LoadBalancer services in bare-metal environments for external access, and provides the same user experience as cloud-based load balancers.       New Feature Request If you have any new ideas or suggestions, please submit a proposal.\n   Contributions Welcome! Find something to work on from GitHub Issues. New contributors are always welcome!\nRead more …\n   Follow Us on Twitter! Obtain up-to-date information about Porter.\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/","title":"Goldydocs"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hu394e7663bf611d6303fc6d5151c4565c_47376_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hu394e7663bf611d6303fc6d5151c4565c_47376_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  Who uses Porter Porter is being used in several production and testing clusters, by several individuals and enterprises        Benlai.com: Expose Kubernetes Service using Porter on Bare Metal Architect Mr.Chen from Benlai.com: \"We use Porter to establish a BGP connection with the cluster's border router in production for several months. Porter is also very easy to integrate with KubeSphere seamlessly\"     QingCloud: Integrate Porter to Provide Load-Balancer service for Customers      This is another section     -- ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/about/","title":"Who uses Porter"}]